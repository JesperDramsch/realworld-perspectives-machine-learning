<p><strong>This is a workshop organised on December 2nd, during <a href="/pydata/">Pydata Global 2022</a> with financial support by the <a href="https://www.software.ac.uk/about/fellows/jesper-soren-dramsch">Software Sustainability Institute</a>.</strong></p>
<p>Check out the <a href="/talks">talks</a>, <a href="/speaker">speakers</a>, and <a href="/schedule">schedule</a>, as well as additional <a href="/resources">resources</a>.</p>
<p><em>ðŸŽ«Get your <a href="/tickets/">tickets here ðŸŽ«</a>.</em></p>
<h2>Description</h2>
<p>Numerous scientific disciplines have noticed a reproducibility crisis of published results. While this important topic was being addressed, the danger of non-reproducible and unsustainable research artefacts using machine learning in science arose. The brunt of this has been avoided by better education of reviewers who nowadays have the skills to spot insufficient validation practices. However, there is more potential to further ease the review process, improve collaboration and make results and models available to fellow scientists. This workshop will teach practical lessons that can be directly applied to elevate the quality of ML applications in science by scientists.</p>
<p>It seems like we avoided the worst signs of the reproducibility crisis in science when applying machine learning in science. Thanks to better education for reviewers, easier access to tools, and a better understanding of zero-knowledge models.</p>
<p>However, there is much more potential for ML in science. The real world comes with many pitfalls that make the application of machine learning very promising, but the verification of scientific results is complex. Nevertheless, many open-source contributors in the field have worked hard to develop practices and resources to ease this process.</p>
<p>We discuss pitfalls and solutions in model evaluation, where the choice of appropriate metrics and adequate splits of the data is important. We discuss benchmarks, testing, and machine learning reproducibility, where we go into detail on pipelines. Pipelines are a great showcase to avoid the main reproducibility pitfalls, as well as, a tool to bridge the gap between ML experts and domain scientists. Interaction with domain scientists, involving existing knowledge, and communication are a constant undercurrent in producing trustworthy, validated, and reliable machine learning solutions.</p>
<p>Overall, this workshop relies on existing high-quality resources like the Turing Way, more applied tutorials like <a href="https://dramsch.net/articles/euroscipy-2022/">Jesper Dramschâ€™s Euroscipy tutorial on ML reproducibility</a>, and professional tools like the Ersilia Hub. Where we utilize real-world examples from different scientific disciplines, e.g. weather and biomedicine.</p>
<p>In this workshop, we present a series of talks from invited speakers that are experts in the application of data science and machine learning to real-world applications. Each talk will be followed by an interactive session to take the theory into practical examples the participants can directly implement to improve their own research. Finally, we close on a discussion that invited active participation and engagement with the speakers as a group.</p>
<p><img alt="Logo in Helvetica saying &quot;Metrics &amp; Evaluation &amp; Explanation &amp; Pipelines &amp; Testing &amp; Reproduction.&quot;" src="/images/Ampersands-ML-workshop.png"></p>